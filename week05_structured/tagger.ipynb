{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging with maximum entropy models (10 pts)\n",
    "\n",
    "In this task you will build a maximum entropy model for part-of-speech tagging. As the name suggests, our problem is all about converting a sequence of words into a sequence of part-of-speech tags. \n",
    "<img src=https://i.stack.imgur.com/6pdIT.png width=320>\n",
    "\n",
    "\n",
    "__Your man goal:__ implement the model from [the article you're given](W96-0213.pdf).\n",
    "\n",
    "Unlike previous tasks, this one gives you greater degree of freedom and less automated tests. We provide you with programming interface but nothing more.\n",
    "\n",
    "__A piece of advice:__ there's a lot of objects happening here. If you don't understand why some object is needed, find `def train` function and see how everything is linked together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: reading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data types:\n",
    "# Word: str\n",
    "# Sentence: list of str\n",
    "TaggedWord = collections.namedtuple('TaggedWord', ['text', 'tag'])\n",
    "# TaggedSentence: list of TaggedWord\n",
    "# Tags: list of TaggedWord\n",
    "# TagLattice: list of Tags\n",
    "\n",
    "def read_tagged_sentences(path):\n",
    "    \"\"\"\n",
    "    Read tagged sentences from CoNLL-U file and return array of TaggedSentence (array of lists of TaggedWord).\n",
    "    \"\"\"\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        sentences = list(map(lambda x: x.strip(), filter(lambda x: not x.startswith('# '), f.readlines())))\n",
    "        sentences = [list(group) for k, group in itertools.groupby(sentences, lambda x: x == '') if not k]\n",
    "        \n",
    "        tagged_sentences = []\n",
    "        for sentence in sentences:\n",
    "            tagged_words = []\n",
    "            for word_definition in sentence:\n",
    "                word_parts = word_definition.split('\\t')\n",
    "                tagged_words.append(TaggedWord(text=word_parts[1], tag=word_parts[3]))\n",
    "            tagged_sentences.append(tagged_words)\n",
    "        \n",
    "        return tagged_sentences\n",
    "\n",
    "def write_tagged_sentence(tagged_sentence, f):\n",
    "    \"\"\"\n",
    "    Write tagged sentence in CoNLL-U format to file-like object f.\n",
    "    \"\"\"\n",
    "    get_word_def = lambda i: f'{i + 1}\\t{tagged_sentence[i].text}\\t_\\t{tagged_sentence[i].tag}' + '\\t_'*6 + '\\n'\n",
    "    f.writelines(map(get_word_def, range(len(tagged_sentence))))\n",
    "\n",
    "def read_tags(path):\n",
    "    \"\"\"\n",
    "    Read a list of possible tags from file and return the list.\n",
    "    \"\"\"\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        return list(filter(lambda x: x != '', f.read().split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert read_tags('data/tags') == ['NOUN','PUNCT','VERB','PRON','ADP','DET','PROPN','ADJ','AUX','ADV','CCONJ','PART','NUM','SCONJ','X','INTJ','SYM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sentences = read_tagged_sentences('data/en-ud-train.conllu')\n",
    "with open('temp', 'w+') as f:\n",
    "    write_tagged_sentence(tagged_sentences[0], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: evaluation\n",
    "\n",
    "We want you to estimate tagging quality by a simple accuracy: a fraction of tag predictions that turned out to be correct - averaged over the entire training corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data types:\n",
    "TaggingQuality = collections.namedtuple('TaggingQuality', ['acc'])\n",
    "\n",
    "def tagging_quality(ref, out):\n",
    "    \"\"\"\n",
    "    Compute tagging quality and reutrn TaggingQuality object.\n",
    "    \"\"\"\n",
    "    nwords = 0\n",
    "    ncorrect = 0\n",
    "    for ref_sentence, out_sentence in itertools.zip_longest(ref, out):\n",
    "        for ref_word, out_word in itertools.zip_longest(ref_sentence, out_sentence):\n",
    "            if ref_word and out_word and ref_word.tag == out_word.tag:\n",
    "                ncorrect += 1\n",
    "            nwords += 1\n",
    "    return ncorrect / nwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Value and Update\n",
    "\n",
    "In order to implement two interlinked data structures: \n",
    "* __Value__ - a class that holds POS tagger's parameters. Basically an array of numbers\n",
    "* __Update__ - a class that stores updates for Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        Dense object that holds parameters.\n",
    "        :param n: array length\n",
    "        \"\"\"\n",
    "        self.positions = np.arange(n)\n",
    "        self.values = np.zeros(n)\n",
    "\n",
    "    def dot(self, update):\n",
    "        if not isinstance(update, Update):\n",
    "            raise ValueError(f'Expected update to be Update, got {type(update)}')\n",
    "        return self.values[update.positions].dot(update.values)\n",
    "\n",
    "    def assign(self, other):\n",
    "        \"\"\"\n",
    "        self = other\n",
    "        other is Value.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, Value):\n",
    "            raise ValueError(f'Expected other to be Value, got {type(other)}')\n",
    "        self.values = np.array(other.values)\n",
    "\n",
    "    def assign_mul(self, coeff):\n",
    "        \"\"\"\n",
    "        self = self * coeff\n",
    "        coeff is float.\n",
    "        \"\"\"\n",
    "        self.values *= coeff\n",
    "\n",
    "    def assign_madd(self, x, coeff):\n",
    "        \"\"\"\n",
    "        self = self + x * coeff\n",
    "        x can be either Value or Update.\n",
    "        coeff is float.\n",
    "        \"\"\"\n",
    "        if not isinstance(x, Value) and not isinstance(x, Update):\n",
    "            raise ValueError(f'Expected x to be Value or Update, got {type(other)}')\n",
    "        if len(x.positions) == 0:\n",
    "            return\n",
    "        self.values[x.positions] += x.values*coeff\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Value({self.positions}, {self.values})'\n",
    "\n",
    "class Update:\n",
    "    \"\"\"\n",
    "    Sparse object that holds an update of parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, positions=None, values=None):\n",
    "        \"\"\"\n",
    "        positions: array of int\n",
    "        values: array of float\n",
    "        \"\"\"\n",
    "        self.positions = np.array(positions if positions else [], dtype=int)\n",
    "        self.values = np.array(values if values else [])\n",
    "        if self.positions.shape != self.values.shape:\n",
    "            raise ValueError(f'Expected positions and values shape to be equal, got: {self.positions.shape}, {self.values.shape}')\n",
    "\n",
    "    def assign_mul(self, coeff):\n",
    "        \"\"\"\n",
    "        self = self * coeff\n",
    "        coeff: float\n",
    "        \"\"\"\n",
    "        self.values *= coeff\n",
    "\n",
    "    def assign_madd(self, update, coeff):\n",
    "        \"\"\"\n",
    "        self = self + update * coeff\n",
    "        coeff: float\n",
    "        \"\"\"\n",
    "        if not isinstance(update, Update):\n",
    "            raise ValueError(f'Expected update to be Update, got {type(update)}')\n",
    "        \n",
    "        new_positions = np.hstack([self.positions, update.positions])\n",
    "        new_values = np.hstack([self.values, update.values*coeff])\n",
    "        \n",
    "        pos_to_value = collections.defaultdict(float)\n",
    "        for position, value in zip(new_positions, new_values):\n",
    "            pos_to_value[position] += value\n",
    "        \n",
    "        pos_and_value = [x for x in pos_to_value.items() if x[1] != 0]\n",
    "        positions, values = ([], []) if len(pos_and_value) == 0 else zip(*pos_and_value)\n",
    "        \n",
    "        self.positions = np.array(positions, dtype=int)\n",
    "        self.values = np.array(values)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Update({self.positions}, {self.values})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = Value(5)\n",
    "assert (v.positions == np.array([0, 1, 2, 3, 4])).all()\n",
    "assert (v.values == np.array([0, 0, 0, 0, 0])).all()\n",
    "\n",
    "other = Value(5)\n",
    "other.values = np.array([1, 2, 1, 2, 2])\n",
    "v.assign(other)\n",
    "assert (v.values == np.array([1, 2, 1, 2, 2])).all()\n",
    "\n",
    "v.assign_mul(2)\n",
    "assert (v.values == np.array([2, 4, 2, 4, 4])).all()\n",
    "assert (other.values == np.array([1, 2, 1, 2, 2])).all()\n",
    "\n",
    "v.assign_madd(other, 3)\n",
    "assert (v.values == np.array([5, 10, 5, 10, 10])).all()\n",
    "\n",
    "u = Update([1, 2], [3,2])\n",
    "assert v.dot(u) == 40\n",
    "\n",
    "v.values = np.array([5, 30, 10, 10, 10])\n",
    "v.assign_madd(u, 2)\n",
    "assert (v.values == np.array([5, 36, 14, 10, 10])).all()\n",
    "\n",
    "u.assign_mul(2)\n",
    "assert (u.values == np.array([6, 4])).all()\n",
    "\n",
    "u.assign_madd(Update([2, 3], [4, 5]), 2)\n",
    "assert (u.positions == np.array([1, 2, 3])).all()\n",
    "assert (u.values == np.array([6, 12, 10])).all()\n",
    "u.assign_madd(Update([0, 1, 0], [1, -1, 1]), 2)\n",
    "assert (u.positions == np.array([1, 2, 3, 0])).all()\n",
    "assert (u.values == np.array([4, 12, 10, 4])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v.assign_madd(Update(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IV: Maximum Entropy POS Tagger\n",
    "_step 1 - draw an oval; step 2 - draw the rest of the owl (c)_\n",
    "\n",
    "In this secion you will implement a simple linear model to predict POS tags.\n",
    "Make sure you [read the article](W96-0213.pdf) before you proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Types:\n",
    "Feature = collections.namedtuple('Feature', ['name', 'index', 'value', 'tag'])\n",
    "Features = Update\n",
    "Hypo = collections.namedtuple('Hypo', ['prev', 'pos', 'tagged_word', 'score'])\n",
    "# prev: previous Hypo\n",
    "# pos: position of word (0-based)\n",
    "# tagged_word: tagging of source_sentence[pos]\n",
    "# score: sum of scores over edges\n",
    "\n",
    "TaggerParams = collections.namedtuple('FeatureParams', [\n",
    "    'src_window',\n",
    "    'dst_order',\n",
    "    'max_suffix',\n",
    "    'beam_size',\n",
    "    'nparams'\n",
    "    ])\n",
    "\n",
    "#import cityhash\n",
    "def h(x):\n",
    "    \"\"\"\n",
    "    Compute CityHash of any object.\n",
    "    Can be used to construct features.\n",
    "    \"\"\"\n",
    "    return hash(repr(x))\n",
    "    #return cityhash.CityHash64(repr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    \"\"\"\n",
    "    A thing that computes score and gradient for given features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self._params = Value(n)\n",
    "\n",
    "    def params(self):\n",
    "        return self._params\n",
    "\n",
    "    def score(self, features):\n",
    "        \"\"\"\n",
    "        features: Update\n",
    "        \"\"\"\n",
    "        return self._params.dot(features)\n",
    "\n",
    "    def gradient(self, features, score):\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureComputer:\n",
    "    def __init__(self, tagger_params, source_sentence):\n",
    "        self.tagger_params = tagger_params\n",
    "        self.source_sentence = source_sentence\n",
    "        self.source_len = len(source_sentence)\n",
    "    \n",
    "    def get_by_index(self, index):\n",
    "        return TaggedWord(None, None) if index < 0 or index >= self.source_len else self.source_sentence[index]\n",
    "    \n",
    "    def compute_features(self, hypo, debug=False):\n",
    "        \"\"\"\n",
    "        Compute features for a given Hypo and return Update.\n",
    "        \"\"\"\n",
    "        word = hypo.tagged_word.text\n",
    "        tag = hypo.tagged_word.tag\n",
    "        pos = hypo.pos\n",
    "        \n",
    "        features = []\n",
    "        features.append(Feature(name='word', index=0, value=word, tag=tag))\n",
    "        \n",
    "        for i in range(1, self.tagger_params.src_window + 1):\n",
    "            features.append(Feature(name='word', index=-i, value=self.get_by_index(pos - i).text, tag=tag))\n",
    "        \n",
    "        for i in range(1, self.tagger_params.src_window + 1):\n",
    "            features.append(Feature(name='word', index=i, value=self.get_by_index(pos + i).text, tag=tag))\n",
    "    \n",
    "        for i in range(1, self.tagger_params.dst_order):\n",
    "            indices = tuple(range(-i, 0))\n",
    "            tags = tuple(map(lambda x: self.get_by_index(pos + x).tag, indices))\n",
    "            features.append(Feature(name='tag', index=indices, value=tags, tag=tag))\n",
    "        \n",
    "        for i in range(1, self.tagger_params.max_suffix + 1):\n",
    "            features.append(Feature(name='prefix', index=i, value=word[:i], tag=tag))\n",
    "        \n",
    "        for i in range(1, self.tagger_params.max_suffix + 1):\n",
    "            features.append(Feature(name='suffix', index=i, value=word[-i:], tag=tag))\n",
    "        \n",
    "        features.append(Feature(name='contains number', index=None, value=any(c.isdigit() for c in word), tag=tag))\n",
    "        features.append(Feature(name='contains uppercase character', index=None, value=any(c.isupper() for c in word), tag=tag))\n",
    "        features.append(Feature(name='contains hyphen', index=None, value='-' in word, tag=tag))\n",
    "        \n",
    "        if debug:\n",
    "            print('\\n'.join(map(repr, features)))\n",
    "        \n",
    "        hashed_features = list(map(lambda x: h(x) % self.tagger_params.nparams, features))\n",
    "        return Update(positions=hashed_features, values=[1]*len(hashed_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature(name='word', index=0, value='about', tag='IN')\n",
      "Feature(name='word', index=-1, value='stories', tag='IN')\n",
      "Feature(name='word', index=-2, value='the', tag='IN')\n",
      "Feature(name='word', index=1, value='well-heeled', tag='IN')\n",
      "Feature(name='word', index=2, value='communities', tag='IN')\n",
      "Feature(name='tag', index=(-1,), value=('NNS',), tag='IN')\n",
      "Feature(name='tag', index=(-2, -1), value=('DT', 'NNS'), tag='IN')\n",
      "Feature(name='prefix', index=1, value='a', tag='IN')\n",
      "Feature(name='prefix', index=2, value='ab', tag='IN')\n",
      "Feature(name='prefix', index=3, value='abo', tag='IN')\n",
      "Feature(name='prefix', index=4, value='abou', tag='IN')\n",
      "Feature(name='suffix', index=1, value='t', tag='IN')\n",
      "Feature(name='suffix', index=2, value='ut', tag='IN')\n",
      "Feature(name='suffix', index=3, value='out', tag='IN')\n",
      "Feature(name='suffix', index=4, value='bout', tag='IN')\n",
      "Feature(name='contains number', index=None, value=False, tag='IN')\n",
      "Feature(name='contains uppercase character', index=None, value=False, tag='IN')\n",
      "Feature(name='contains hyphen', index=None, value=False, tag='IN')\n",
      "\n",
      "Feature(name='word', index=0, value='well-heeled', tag='JJ')\n",
      "Feature(name='word', index=-1, value='about', tag='JJ')\n",
      "Feature(name='word', index=-2, value='stories', tag='JJ')\n",
      "Feature(name='word', index=1, value='communities', tag='JJ')\n",
      "Feature(name='word', index=2, value='and', tag='JJ')\n",
      "Feature(name='tag', index=(-1,), value=('IN',), tag='JJ')\n",
      "Feature(name='tag', index=(-2, -1), value=('NNS', 'IN'), tag='JJ')\n",
      "Feature(name='prefix', index=1, value='w', tag='JJ')\n",
      "Feature(name='prefix', index=2, value='we', tag='JJ')\n",
      "Feature(name='prefix', index=3, value='wel', tag='JJ')\n",
      "Feature(name='prefix', index=4, value='well', tag='JJ')\n",
      "Feature(name='suffix', index=1, value='d', tag='JJ')\n",
      "Feature(name='suffix', index=2, value='ed', tag='JJ')\n",
      "Feature(name='suffix', index=3, value='led', tag='JJ')\n",
      "Feature(name='suffix', index=4, value='eled', tag='JJ')\n",
      "Feature(name='contains number', index=None, value=False, tag='JJ')\n",
      "Feature(name='contains uppercase character', index=None, value=False, tag='JJ')\n",
      "Feature(name='contains hyphen', index=None, value=True, tag='JJ')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Update([13  5  8  3  7  8  5 10  8  0  0  5  4 13  6  2  6  4], [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = ['the', 'stories', 'about', 'well-heeled', 'communities', 'and', 'developers']\n",
    "tags = ['DT', 'NNS', 'IN', 'JJ', 'NNS', 'CC', 'NNS']\n",
    "\n",
    "tagged_sentence = [TaggedWord(text=sentence[i], tag=tags[i]) for i in range(len(sentence))]\n",
    "\n",
    "hypos = []\n",
    "for i, tagged_word in enumerate(tagged_sentence):\n",
    "    hypos.append(Hypo(prev=None if i == 0 else hypos[i - 1], pos=i, tagged_word=tagged_word, score=0.5))\n",
    "\n",
    "tagger_params = TaggerParams(src_window=2, dst_order=3, max_suffix=4, beam_size=4, nparams=16)\n",
    "fc = FeatureComputer(tagger_params, tagged_sentence)\n",
    "fc.compute_features(hypos[2], debug=True)\n",
    "print()\n",
    "fc.compute_features(hypos[3], debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part V: Beam search\n",
    "\n",
    "We can find the most likely tagging approximately using Beam Search. As everything else, it comes with a separate interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def with_score(hypo, score):\n",
    "    return Hypo(prev=hypo.prev, pos=hypo.pos, tagged_word=hypo.tagged_word, score=score)\n",
    "\n",
    "class BeamSearchTask:\n",
    "    \"\"\"\n",
    "    An abstract beam search task. Can be used with beam_search() generic \n",
    "    function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tagger_params, source_sentence, model, tags):\n",
    "        self.tagger_params = tagger_params\n",
    "        self.source_sentence = source_sentence\n",
    "        self.model = model\n",
    "        self.tags = tags\n",
    "        self.feature_computer = FeatureComputer(tagger_params, source_sentence)\n",
    "\n",
    "    def total_num_steps(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses between beginning and end (number of words in\n",
    "        the sentence).\n",
    "        \"\"\"\n",
    "        return len(self.source_sentence)\n",
    "\n",
    "    def beam_size(self):\n",
    "        return self.tagger_params.beam_size\n",
    "\n",
    "    def expand(self, hypo):\n",
    "        \"\"\"\n",
    "        Given Hypo, return a list of its possible expansions.\n",
    "        'hypo' might be None -- return a list of initial hypos then.\n",
    "\n",
    "        Compute hypotheses' scores inside this function!\n",
    "        \"\"\"\n",
    "        prev_score = hypo.score if hypo else 0\n",
    "        new_pos = hypo.pos + 1 if hypo else 0\n",
    "        \n",
    "        hypos = []\n",
    "        if new_pos == len(self.source_sentence):\n",
    "            return hypos\n",
    "        \n",
    "        for tag in self.tags:\n",
    "            word = TaggedWord(self.source_sentence[new_pos].text, tag)\n",
    "            new_hypo = Hypo(prev=hypo, pos=new_pos, tagged_word=word, score=0)\n",
    "            score = self.model.score(self.feature_computer.compute_features(new_hypo))\n",
    "            hypos.append(with_score(new_hypo, prev_score + score))\n",
    "            \n",
    "        return hypos\n",
    "\n",
    "    def recombo_hash(self, hypo, debug=False):\n",
    "        \"\"\"\n",
    "        If two hypos have the same recombination hashes, they can be collapsed\n",
    "        together, leaving only the hypothesis with a better score.\n",
    "        \"\"\"\n",
    "        tags = []\n",
    "        for i in range(self.tagger_params.dst_order):\n",
    "            tags.append(hypo.tagged_word.tag if hypo else None)\n",
    "            hypo = hypo.prev if hypo else None\n",
    "        \n",
    "        if debug:\n",
    "            print(tags)\n",
    "            \n",
    "        return h(tuple(tags))\n",
    "\n",
    "\n",
    "def beam_search(beam_search_task):\n",
    "    \"\"\"\n",
    "    Return list of stacks.\n",
    "    Each stack contains several hypos, sorted by score in descending \n",
    "    order (i.e. better hypos first).\n",
    "    \"\"\"\n",
    "    beam_size = beam_search_task.beam_size()\n",
    "    n_steps = beam_search_task.total_num_steps()\n",
    "    stacks = [[None]]\n",
    "    for _ in range(n_steps):\n",
    "        new_hypos = []\n",
    "        for prev_hypo in stacks[-1]:\n",
    "            new_hypos.extend(beam_search_task.expand(prev_hypo))\n",
    "            \n",
    "        unique_hash_hypos = []\n",
    "        for _, same_hash_hypos in itertools.groupby(new_hypos, beam_search_task.recombo_hash):\n",
    "            unique_hash_hypos.append(max(same_hash_hypos, key=lambda x: x.score))\n",
    "            \n",
    "        stacks.append(sorted(unique_hash_hypos, key=lambda x: -x.score)[:beam_size])\n",
    "        \n",
    "    return stacks[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Hypo(prev=None, pos=0, tagged_word=TaggedWord(text='the', tag='DT'), score=0.0),\n",
       " Hypo(prev=None, pos=0, tagged_word=TaggedWord(text='the', tag='NNS'), score=0.0),\n",
       " Hypo(prev=None, pos=0, tagged_word=TaggedWord(text='the', tag='IN'), score=0.0),\n",
       " Hypo(prev=None, pos=0, tagged_word=TaggedWord(text='the', tag='JJ'), score=0.0),\n",
       " Hypo(prev=None, pos=0, tagged_word=TaggedWord(text='the', tag='NNS'), score=0.0),\n",
       " Hypo(prev=None, pos=0, tagged_word=TaggedWord(text='the', tag='CC'), score=0.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearModel(tagger_params.nparams)\n",
    "bst = BeamSearchTask(tagger_params, tagged_sentence, model, tags[:6])\n",
    "bst.expand(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DT', None, None]\n",
      "['DT', 'DT', None]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-735786781911998934, -2662653630755002111)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hypo = lambda prev, i: Hypo(prev=prev, pos=i, tagged_word=TaggedWord(None, 'DT'), score=0)\n",
    "bst.recombo_hash(get_hypo(None, 0), debug=True), bst.recombo_hash(get_hypo(get_hypo(None, 0), 1), debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Hypo(prev=Hypo(prev=Hypo(prev=None, pos=0, tagged_word=TaggedWord(text='the', tag='DT'), score=0.0), pos=1, tagged_word=TaggedWord(text='stories', tag='DT'), score=0.0), pos=2, tagged_word=TaggedWord(text='about', tag='DT'), score=0.0),\n",
       " Hypo(prev=Hypo(prev=Hypo(prev=None, pos=0, tagged_word=TaggedWord(text='the', tag='DT'), score=0.0), pos=1, tagged_word=TaggedWord(text='stories', tag='DT'), score=0.0), pos=2, tagged_word=TaggedWord(text='about', tag='NNS'), score=0.0),\n",
       " Hypo(prev=Hypo(prev=Hypo(prev=None, pos=0, tagged_word=TaggedWord(text='the', tag='DT'), score=0.0), pos=1, tagged_word=TaggedWord(text='stories', tag='DT'), score=0.0), pos=2, tagged_word=TaggedWord(text='about', tag='IN'), score=0.0),\n",
       " Hypo(prev=Hypo(prev=Hypo(prev=None, pos=0, tagged_word=TaggedWord(text='the', tag='DT'), score=0.0), pos=1, tagged_word=TaggedWord(text='stories', tag='DT'), score=0.0), pos=2, tagged_word=TaggedWord(text='about', tag='JJ'), score=0.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacks = beam_search(bst)\n",
    "stacks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_sentences(dataset, tagger_params, model, tags):\n",
    "    \"\"\"\n",
    "    Main predict function.\n",
    "    Tags all sentences in dataset. Dataset is a list of TaggedSentence; while \n",
    "    tagging, ignore existing tags.\n",
    "    \"\"\"\n",
    "    tagged_dataset = []\n",
    "    for sentence in dataset:\n",
    "        bst = BeamSearchTask(tagger_params, sentence, model, tags)\n",
    "        stacks = beam_search(bst)\n",
    "        best_hypo = stacks[-1][0]\n",
    "        \n",
    "        tagged_sentence = []\n",
    "        for _ in range(len(sentence)):\n",
    "            tagged_sentence.append(best_hypo.tagged_word)\n",
    "            best_hypo = best_hypo.prev\n",
    "        \n",
    "        tagged_dataset.append(list(reversed(tagged_sentence)))\n",
    "        \n",
    "    return tagged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[TaggedWord(text='the', tag='DT'),\n",
       "  TaggedWord(text='stories', tag='DT'),\n",
       "  TaggedWord(text='about', tag='DT'),\n",
       "  TaggedWord(text='well-heeled', tag='DT'),\n",
       "  TaggedWord(text='communities', tag='DT'),\n",
       "  TaggedWord(text='and', tag='DT'),\n",
       "  TaggedWord(text='developers', tag='DT')]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_sentences([tagged_sentence], tagger_params, model, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VI: Optimization objective and algorithm\n",
    "\n",
    "Once we defined our model and inference algorithm, we can define an optimization task: an object that computes loss function and its gradients w.r.t. model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OptimizationTask:\n",
    "    \"\"\"\n",
    "    Optimization task that can be used with sgd().\n",
    "    \"\"\"\n",
    "\n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        Parameters which are optimized in this optimization task.\n",
    "        Return Value.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        \"\"\"\n",
    "        Return (loss, gradient) on a specific example.\n",
    "\n",
    "        loss: float\n",
    "        gradient: Update\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class UnstructuredPerceptronOptimizationTask(OptimizationTask):\n",
    "    def __init__(self, tagger_params, tags):\n",
    "        self.tagger_params = TaggerParams(src_window=tagger_params.src_window, dst_order=tagger_params.dst_order,\n",
    "                                          max_suffix=tagger_params.max_suffix, beam_size=len(tags),\n",
    "                                          nparams=tagger_params.nparams)\n",
    "        self.model = LinearModel(tagger_params.nparams)\n",
    "        self.tags = tags\n",
    "\n",
    "    def params(self):\n",
    "        return self.model.params()\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        beam_search_task = BeamSearchTask(self.tagger_params, golden_sentence, self.model, self.tags)\n",
    "        golden_hypo = None\n",
    "        feature_computer = FeatureComputer(self.tagger_params, golden_sentence)\n",
    "        loss = 0\n",
    "        grad = Update()\n",
    "        for i in range(len(golden_sentence)):\n",
    "            new_hypos = beam_search_task.expand(golden_hypo)\n",
    "            \n",
    "            rival_hypo = max(new_hypos, key=lambda x: x.score)\n",
    "            golden_hypo = [x for x in new_hypos if x.tagged_word.tag == golden_sentence[i].tag][0]\n",
    "\n",
    "            rival_features = feature_computer.compute_features(rival_hypo)\n",
    "            grad.assign_madd(self.model.gradient(rival_features, score=None), 1)\n",
    "            \n",
    "            golden_features = feature_computer.compute_features(golden_hypo)\n",
    "            grad.assign_madd(self.model.gradient(golden_features, score=None), -1)\n",
    "            \n",
    "            loss += rival_hypo.score - golden_hypo.score\n",
    "            \n",
    "        return loss, grad\n",
    "        \n",
    "class StructuredPerceptronOptimizationTask(OptimizationTask):\n",
    "    def __init__(self, tagger_params, tags):\n",
    "        self.tagger_params = tagger_params\n",
    "        self.model = LinearModel(tagger_params.nparams)\n",
    "        self.tags = tags\n",
    "\n",
    "    def params(self):\n",
    "        return self.model.params()\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        # Do beam search.\n",
    "        beam_search_task = BeamSearchTask(self.tagger_params, golden_sentence, self.model, self.tags)\n",
    "        stacks = beam_search(beam_search_task)\n",
    "\n",
    "        # Compute chain of golden hypos (and their scores!).\n",
    "        golden_hypo = None\n",
    "        hypos = []\n",
    "        feature_computer = FeatureComputer(self.tagger_params, golden_sentence)\n",
    "        for i in range(len(golden_sentence)):\n",
    "            new_golden_hypo = Hypo(prev=golden_hypo, pos=i, tagged_word=golden_sentence[i], score=0)\n",
    "            new_hypo_score = self.model.score(feature_computer.compute_features(new_golden_hypo))\n",
    "            golden_hypo = with_score(new_golden_hypo, new_hypo_score)\n",
    "            hypos.append(golden_hypo)\n",
    "\n",
    "        # Find where to update.\n",
    "        golden_head = None\n",
    "        rival_head = None\n",
    "        for i, hypo in enumerate(hypos):\n",
    "            if hypo.score < stacks[i][-1].score:\n",
    "                golden_head = hypo\n",
    "                rival_head = stacks[i][0]\n",
    "        \n",
    "        if golden_head is None and rival_head is None:\n",
    "            golden_head = hypos[-1] \n",
    "            rival_head = stacks[-1][0]\n",
    "\n",
    "        # Compute gradient.\n",
    "        loss = 0\n",
    "        grad = Update()\n",
    "        while golden_head and rival_head:\n",
    "            rival_features = feature_computer.compute_features(rival_head)\n",
    "            grad.assign_madd(self.model.gradient(rival_features, score=None), 1)\n",
    "\n",
    "            golden_features = feature_computer.compute_features(golden_head)\n",
    "            grad.assign_madd(self.model.gradient(golden_features, score=None), -1)\n",
    "\n",
    "            loss += rival_head.score - golden_head.score\n",
    "            \n",
    "            golden_head = golden_head.prev\n",
    "            rival_head = rival_head.prev\n",
    "\n",
    "        return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " Update([ 5 12  4  1 14  3 10  7  2  9  8  6  0 13], [ 3.  4.  4. -2. -6.  3.  6. -8. -6.  3.  5.  1. -5. -2.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UnstructuredPerceptronOptimizationTask(tagger_params, tags).loss_and_gradient(tagged_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " Update([ 7  4  8  5 12  0  9 10  2  3 14  1  6 13], [-8.  4.  5.  3.  4. -5.  3.  6. -6.  3. -6. -2.  1. -2.]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StructuredPerceptronOptimizationTask(tagger_params, tags).loss_and_gradient(tagged_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VII: optimizer\n",
    "\n",
    "By this point we can define a model with parameters $\\theta$ and a problem that computes gradients $ \\partial L \\over \\partial \\theta $ w.r.t. model parameters.\n",
    "\n",
    "Optimization is performed by gradient descent: $ \\theta := \\theta - \\alpha {\\partial L \\over \\partial \\theta} $\n",
    "\n",
    "In order to speed up training, we use stochastic gradient descent that operates on minibatches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "SGDParams = collections.namedtuple('SGDParams', [\n",
    "    'epochs',\n",
    "    'learning_rate',\n",
    "    'minibatch_size',\n",
    "    'average' # bool or int\n",
    "    ])\n",
    "\n",
    "\n",
    "def make_batches(dataset, n_batches):\n",
    "    \"\"\"\n",
    "    Make list of batches from a list of examples.\n",
    "    \"\"\"\n",
    "    return np.array_split(np.random.permutation(dataset), n_batches)\n",
    "\n",
    "\n",
    "def sgd(sgd_params, optimization_task, dataset, after_each_epoch_fn, after_each_batch_fn=None):\n",
    "    \"\"\"\n",
    "    Run (averaged) SGD on a generic optimization task. Modify optimization\n",
    "    task's parameters.\n",
    "\n",
    "    After each epoch (and also before and after the whole training),\n",
    "    run after_each_epoch_fn().\n",
    "    \"\"\"\n",
    "    after_each_epoch_fn(stage='Initial quality')\n",
    "    if sgd_params.average:\n",
    "        params_sum = copy.deepcopy(optimization_task.params())\n",
    "    \n",
    "    n_batch = len(dataset)//sgd_params.minibatch_size + 1\n",
    "    callback_params = {'n_epoch': sgd_params.epochs, 'n_batch': n_batch}\n",
    "    for i in range(sgd_params.epochs):\n",
    "        callback_params['epoch_i'] = i\n",
    "        callback_params['epoch_start'] = time.time()\n",
    "        for j, batch in enumerate(make_batches(dataset, n_batch)):\n",
    "            callback_params['batch_i'] = j\n",
    "            callback_params['batch_start'] = time.time()\n",
    "            loss_sum = 0\n",
    "            grad_sum = Update()\n",
    "            for sentence in batch:\n",
    "                loss, grad = optimization_task.loss_and_gradient(sentence)\n",
    "                grad_sum.assign_madd(grad, 1)\n",
    "                loss_sum += loss\n",
    "            grad_avg = grad_sum\n",
    "            grad_avg.assign_mul(1/sgd_params.minibatch_size)\n",
    "            loss_avg = loss_sum/sgd_params.minibatch_size\n",
    "            optimization_task.params().assign_madd(grad_avg, -sgd_params.learning_rate)\n",
    "            if sgd_params.average:\n",
    "                params_sum.assign_madd(optimization_task.params(), 1)\n",
    "            callback_params['batch_end'] = time.time()\n",
    "            callback_params['batch_loss'] = loss_avg\n",
    "            if after_each_batch_fn:\n",
    "                after_each_batch_fn(**callback_params)\n",
    "        callback_params['epoch_end'] = time.time()\n",
    "        after_each_epoch_fn(**callback_params)\n",
    "    if sgd_params.average:\n",
    "        params_avg = params_sum\n",
    "        params_avg.assign_mul(1/(sgd_params.epochs*(len(dataset)//sgd_params.minibatch_size + 1)))\n",
    "        optimization_task.params().assign(params_avg)\n",
    "    after_each_epoch_fn(stage='Final quality')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VIII: Training loop\n",
    "\n",
    "The train function combines everthing you used below to get new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pickle\n",
    "\n",
    "def train(\n",
    "    tags='./data/tags',\n",
    "    train_dataset='./data/en-ud-train.conllu',\n",
    "    dev_dataset='./data/en-ud-dev.conllu',\n",
    "    model_name='./model.npz',\n",
    "    \n",
    "    sgd_epochs=15,\n",
    "    sgd_learning_rate=0.01,\n",
    "    sgd_minibatch_size=32,\n",
    "    sgd_average=True,\n",
    "    \n",
    "    # Number of context tags in output tagging to use for features\n",
    "    tagger_src_window=2,\n",
    "    \n",
    "    # Number of context tags in output tagging to use for features\n",
    "    tagger_dst_order=3,\n",
    "    \n",
    "    # Maximal number of prefix/suffix letters to use for features\n",
    "    tagger_max_suffix=4,\n",
    "    \n",
    "    # Width for beam search (0 means unstructured)\n",
    "    beam_size=1,\n",
    "    \n",
    "    # Parameter vector size (for hashing)\n",
    "    nparams= 2 ** 22,\n",
    "):\n",
    "    \"\"\" Train a pos-tagger model and save it's parameters to :model: \"\"\"\n",
    "\n",
    "    # Beam size.\n",
    "    optimization_task_cls = StructuredPerceptronOptimizationTask\n",
    "    if beam_size == 0:\n",
    "        beam_size = 1\n",
    "        optimization_task_cls = UnstructuredPerceptronOptimizationTask\n",
    "\n",
    "    # Parse cmdargs.\n",
    "    tags = read_tags(tags)\n",
    "    train_dataset = read_tagged_sentences(train_dataset)\n",
    "    dev_dataset = read_tagged_sentences(dev_dataset)\n",
    "    params = None\n",
    "    if os.path.exists(model_name):\n",
    "        params = pickle.load(open(model_name, 'rb'))\n",
    "    sgd_params = SGDParams(\n",
    "        epochs=sgd_epochs,\n",
    "        learning_rate=sgd_learning_rate,\n",
    "        minibatch_size=sgd_minibatch_size,\n",
    "        average=sgd_average\n",
    "        )\n",
    "    tagger_params = TaggerParams(\n",
    "        src_window=tagger_src_window,\n",
    "        dst_order=tagger_dst_order,\n",
    "        max_suffix=tagger_max_suffix,\n",
    "        beam_size=beam_size,\n",
    "        nparams=nparams\n",
    "        )\n",
    "\n",
    "    # Load optimization task\n",
    "    optimization_task = optimization_task_cls(tagger_params, tags)\n",
    "    if params is not None:\n",
    "        print(f'Loading parameters from {model_name}')\n",
    "        optimization_task.params().assign(params)\n",
    "\n",
    "    # Validation.\n",
    "    def after_each_epoch_fn(*args, **kwargs):\n",
    "        model = LinearModel(nparams)\n",
    "        model.params().assign(optimization_task.params())\n",
    "        tagged_sentences = tag_sentences(dev_dataset, tagger_params, model, tags)\n",
    "        quality = tagging_quality(out=tagged_sentences, ref=dev_dataset)\n",
    "        \n",
    "        if 'stage' in kwargs:\n",
    "            stage = kwargs[\"stage\"]\n",
    "            print(f'\\r{stage}: {quality}')\n",
    "        else:\n",
    "            epoch_i = kwargs['epoch_i']\n",
    "            n_epoch = kwargs['n_epoch']\n",
    "            start = kwargs['epoch_start']\n",
    "            end = kwargs['epoch_end']\n",
    "            print(f'\\rEpoch #{epoch_i + 1}/{n_epoch} ({get_batch_string(*args, **kwargs)}), epoch_quality: {quality:.3f}, epoch_time: {(end - start)/60:.3f} min')\n",
    "        \n",
    "        pickle.dump(optimization_task.params(), open(model_name, 'wb'))\n",
    "    \n",
    "    def after_each_batch_fn(*args, **kwargs):\n",
    "        epoch_i = kwargs['epoch_i']\n",
    "        n_epoch = kwargs['n_epoch']\n",
    "        print(f'\\rEpoch #{epoch_i + 1}/{n_epoch} ({get_batch_string(*args, **kwargs)})', end='')\n",
    "    \n",
    "    def get_batch_string(*args, **kwargs):\n",
    "        start = kwargs['batch_start']\n",
    "        end = kwargs['batch_end']\n",
    "        batch_i = kwargs['batch_i']\n",
    "        n_batch = kwargs['n_batch']\n",
    "        batch_loss = kwargs['batch_loss']\n",
    "        return f'batch #{batch_i + 1}/{n_batch}, batch_loss: {batch_loss:.5f}, batch_time: {end - start:.3f} s'\n",
    "    \n",
    "    # Run SGD.\n",
    "    sgd(sgd_params, optimization_task, train_dataset, after_each_epoch_fn, after_each_batch_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial quality: 0.16280971461280563\n",
      "Epoch #1/15 (batch #392/392, batch_loss: 12.74798, batch_time: 2.121 s), epoch_quality: 0.805, epoch_time: 7.134 min\n",
      "Epoch #2/15 (batch #392/392, batch_loss: 10.13711, batch_time: 1.406 s), epoch_quality: 0.918, epoch_time: 8.568 min\n",
      "Epoch #3/15 (batch #392/392, batch_loss: 12.79489, batch_time: 1.263 s), epoch_quality: 0.923, epoch_time: 10.028 min\n",
      "Epoch #4/15 (batch #392/392, batch_loss: 13.49482, batch_time: 2.703 s), epoch_quality: 0.922, epoch_time: 8.920 min\n",
      "Epoch #5/15 (batch #392/392, batch_loss: 6.41115, batch_time: 0.795 s), epoch_quality: 0.919, epoch_time: 6.542 min\n",
      "Epoch #6/15 (batch #392/392, batch_loss: 10.99705, batch_time: 0.934 s), epoch_quality: 0.925, epoch_time: 6.416 min\n",
      "Epoch #7/15 (batch #392/392, batch_loss: 4.24250, batch_time: 0.619 s), epoch_quality: 0.928, epoch_time: 6.319 min\n",
      "Epoch #8/15 (batch #392/392, batch_loss: 7.29345, batch_time: 0.766 s), epoch_quality: 0.931, epoch_time: 6.386 min\n",
      "Epoch #9/15 (batch #392/392, batch_loss: 9.34747, batch_time: 0.984 s), epoch_quality: 0.936, epoch_time: 6.430 min\n",
      "Epoch #10/15 (batch #392/392, batch_loss: 12.72678, batch_time: 1.147 s), epoch_quality: 0.930, epoch_time: 7.143 min\n",
      "Epoch #11/15 (batch #392/392, batch_loss: 11.54467, batch_time: 1.035 s), epoch_quality: 0.929, epoch_time: 7.252 min\n",
      "Epoch #12/15 (batch #392/392, batch_loss: 8.00169, batch_time: 1.022 s), epoch_quality: 0.934, epoch_time: 7.548 min\n",
      "Epoch #13/15 (batch #392/392, batch_loss: 9.05141, batch_time: 0.963 s), epoch_quality: 0.933, epoch_time: 6.915 min\n",
      "Epoch #14/15 (batch #392/392, batch_loss: 11.44396, batch_time: 1.194 s), epoch_quality: 0.921, epoch_time: 7.511 min\n",
      "Epoch #15/15 (batch #392/392, batch_loss: 10.72416, batch_time: 1.191 s), epoch_quality: 0.936, epoch_time: 7.758 min\n",
      "Final quality: 0.9403058304031401\n"
     ]
    }
   ],
   "source": [
    "# train a model with default params\n",
    "train(model_name='./model_default.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IX: Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def test(\n",
    "    tags='./data/tags',\n",
    "    dataset='./data/en-ud-dev.conllu',\n",
    "    model='./model.npz',\n",
    "    \n",
    "    # model and inference params; see train for their description\n",
    "    tagger_src_window=2,\n",
    "    tagger_dst_order=3,\n",
    "    tagger_max_suffix=4,\n",
    "    beam_size=1,\n",
    "    nparams= 2 ** 22,\n",
    "    out_file='./result.conllu',\n",
    "    print_quality=True\n",
    "):\n",
    "\n",
    "\n",
    "    tags = read_tags(tags)\n",
    "    dataset = read_tagged_sentences(dataset)\n",
    "    params = pickle.load(open(model, 'rb'))\n",
    "    tagger_params = TaggerParams(\n",
    "        src_window=tagger_src_window,\n",
    "        dst_order=tagger_dst_order,\n",
    "        max_suffix=tagger_max_suffix,\n",
    "        beam_size=beam_size,\n",
    "        nparams=nparams\n",
    "        )\n",
    "\n",
    "    # Load model.\n",
    "    model = LinearModel(params.values.shape[0])\n",
    "    model.params().assign(params)\n",
    "\n",
    "    # Tag all sentences.\n",
    "    tagged_sentences = tag_sentences(dataset, tagger_params, model, tags)\n",
    "\n",
    "    # Write tagged sentences.\n",
    "    with open(out_file, 'w+', encoding='utf8') as f:\n",
    "        for tagged_sentence in tagged_sentences:\n",
    "            write_tagged_sentence(tagged_sentence, f)\n",
    "\n",
    "    # Measure and print quality.\n",
    "    if print_quality:\n",
    "        q = pprint.pformat(tagging_quality(out=tagged_sentences, ref=dataset))\n",
    "        print(q, file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.9403058304031401\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "test(model='./model_default.npz', out_file='./result_default.conllu')\n",
    "\n",
    "# sanity chec: accuracy > 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tagging\n",
    "test(model='./model_default.npz', dataset='./data/en-ud-test-notags.conllu', out_file='./en-ud-test-tagged.conllu', print_quality=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part X: play with it\n",
    "\n",
    "_This part is optional_\n",
    "\n",
    "Once you've built something, it's only natural to test the limits of your contraption.\n",
    "\n",
    "At minumum, we want you to find out how default model accuracy depends on __beam size__\n",
    "\n",
    "To get maximum points, your model should get final quality >= 93% \n",
    "\n",
    "Any further analysis is welcome, as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_name='./model_unstructured.npz', beam_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.9318014555564641\n"
     ]
    }
   ],
   "source": [
    "test(model='./model_unstructured.npz', out_file='./result_unstructured.conllu', beam_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial quality: 0.16280971461280563\n",
      "Epoch #1/15 (batch #392/392, batch_loss: 8.92628, batch_time: 1.859 s), epoch_quality: 0.851, epoch_time: 12.831 min\n",
      "Epoch #2/15 (batch #392/392, batch_loss: 8.21362, batch_time: 1.717 s), epoch_quality: 0.918, epoch_time: 12.337 min\n",
      "Epoch #3/15 (batch #392/392, batch_loss: 6.90967, batch_time: 1.355 s), epoch_quality: 0.926, epoch_time: 11.991 min\n",
      "Epoch #4/15 (batch #392/392, batch_loss: 14.82606, batch_time: 2.062 s), epoch_quality: 0.753, epoch_time: 11.306 min\n",
      "Epoch #5/15 (batch #388/392, batch_loss: 11.47744, batch_time: 1.706 s)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-e7aa13652cbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./model_beam2.npz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-905a64bc87e0>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(tags, train_dataset, dev_dataset, model_name, sgd_epochs, sgd_learning_rate, sgd_minibatch_size, sgd_average, tagger_src_window, tagger_dst_order, tagger_max_suffix, beam_size, nparams)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;31m# Run SGD.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msgd_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimization_task\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mafter_each_epoch_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mafter_each_batch_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-1a03366bcc8c>\u001b[0m in \u001b[0;36msgd\u001b[1;34m(sgd_params, optimization_task, dataset, after_each_epoch_fn, after_each_batch_fn)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mgrad_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUpdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimization_task\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_and_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[0mgrad_sum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mloss_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-8b629db8ef4e>\u001b[0m in \u001b[0;36mloss_and_gradient\u001b[1;34m(self, golden_sentence)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Do beam search.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mbeam_search_task\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeamSearchTask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagger_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgolden_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mstacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeam_search_task\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;31m# Compute chain of golden hypos (and their scores!).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-6736818754a3>\u001b[0m in \u001b[0;36mbeam_search\u001b[1;34m(beam_search_task)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mnew_hypos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprev_hypo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstacks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mnew_hypos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeam_search_task\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_hypo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0munique_hash_hypos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-6736818754a3>\u001b[0m in \u001b[0;36mexpand\u001b[1;34m(self, hypo)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTaggedWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource_sentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_pos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mnew_hypo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHypo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhypo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagged_word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_computer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_hypo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mhypos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_hypo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_score\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-34ffb653d6a1>\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUpdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \"\"\"\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-f9280c8724de>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(self, update)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUpdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Expected update to be Update, got {type(update)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpositions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# playing with it takes too long\n",
    "train(model_name='./model_beam2.npz', beam_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.7530460381061411\n"
     ]
    }
   ],
   "source": [
    "test(model='./model_beam2.npz', out_file='./result_beam2.conllu', beam_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
