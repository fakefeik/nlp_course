{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Word Alignment Assignment__\n",
    "\n",
    "Your task is to learn word alignments for the data provided with this Python Notebook. \n",
    "\n",
    "Start by running the 'train' function below and implementing the assertions which will fail. Then consider the following improvements to the baseline model:\n",
    "* Is the TranslationModel parameterized efficiently?\n",
    "* What form of PriorModel would help here? (Currently the PriorModel is uniform.)\n",
    "* How could you use a Hidden Markov Model to model word alignment indices? (There's an implementation of simple HMM below to help you start.)\n",
    "* How could you initialize more complex models from simpler ones?\n",
    "* How could you model words that are not aligned to anything?\n",
    "\n",
    "Grades will be assigned as follows*:\n",
    "\n",
    " AER below on blinds   |  Grade \n",
    "----------|-------------\n",
    " 0.5 - 0.6 |   1 \n",
    " 0.4 - 0.5 |   2 \n",
    " 0.35 - 0.4 |  3    \n",
    " 0.3 - 0.35 |  4    \n",
    " 0.25 - 0.3 |  5   \n",
    " \n",
    "You should save the notebook with the final scores for 'dev' and 'test' test sets.\n",
    "\n",
    "*__Note__: Students who submitted a version of this assignment last year will have a 0.05 AER handicap, i.e to get a grade of 5, they will need to get an AER below 0.25.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell contains the generative models that you may want to use for word alignment.\n",
    "# Currently only the TranslationModel is at all functional.\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "class TranslationModel:\n",
    "    \"Models conditional distribution over trg words given a src word.\"\n",
    "\n",
    "    def __init__(self, src_corpus, trg_corpus):\n",
    "        self._trg_given_src_probs = defaultdict(lambda : defaultdict(lambda : 1.0))\n",
    "        self._src_trg_counts = defaultdict(lambda : defaultdict(lambda : 0.0))\n",
    "\n",
    "    def get_params(self):\n",
    "        return self._trg_given_src_probs\n",
    "\n",
    "    def get_conditional_prob(self, src_token, trg_token):\n",
    "        \"Return the conditional probability of trg_token given src_token.\"\n",
    "        return self._trg_given_src_probs[src_token][trg_token]\n",
    "\n",
    "    def get_parameters_for_sentence_pair(self, src_tokens, trg_tokens):\n",
    "        \"Returns matrix with t[i][j] = p(f_j|e_i).\"\n",
    "        return np.array([[self._trg_given_src_probs[src_token][trg_token]\n",
    "                          for trg_token in trg_tokens] for src_token in src_tokens])\n",
    "\n",
    "    def collect_statistics(self, src_tokens, trg_tokens, posterior_matrix):\n",
    "        \"Accumulate counts of translations from: posterior_matrix[j][i] = p(a_j=i|e, f)\"\n",
    "        assert posterior_matrix.shape == (len(trg_tokens), len(src_tokens))\n",
    "        for i, w1 in enumerate(src_tokens):\n",
    "            for j, w2 in enumerate(trg_tokens):\n",
    "                self._src_trg_counts[w1][w2] += posterior_matrix[j][i]\n",
    "        \n",
    "    def recompute_parameters(self):\n",
    "        \"Reestimate parameters and reset counters.\"\n",
    "        self._trg_given_src_probs = defaultdict(lambda : defaultdict(lambda : 0.0))\n",
    "        for w1 in self._src_trg_counts:\n",
    "            sum_ = sum(self._src_trg_counts[w1].values())\n",
    "            for w2, value in self._src_trg_counts[w1].items():\n",
    "                self._trg_given_src_probs[w1][w2] += value/sum_\n",
    "        \n",
    "        self._src_trg_counts = defaultdict(lambda : defaultdict(lambda : 0.0))\n",
    "\n",
    "class PriorModel:\n",
    "    \"Models the prior probability of an alignment given only the sentence lengths and token indices.\"\n",
    "\n",
    "    use_distance = False\n",
    "    normalize_distance = False\n",
    "    dim = 10\n",
    "    \n",
    "    def __init__(self, src_corpus, trg_corpus):\n",
    "        \"Add counters and parameters here for more sophisticated models.\"\n",
    "        if not self.normalize_distance:\n",
    "            self._distance_counts = {}\n",
    "            self._distance_probs = {}\n",
    "        else:\n",
    "            self._distance_counts = np.zeros((self.dim, self.dim))\n",
    "            self._distance_probs = np.ones((self.dim, self.dim))/self.dim\n",
    "\n",
    "    def _clamp(self, index, length):\n",
    "        return int(index/length*self.dim)\n",
    "            \n",
    "    def get_parameters_for_sentence_pair(self, src_length, trg_length):\n",
    "        key = (src_length, trg_length)\n",
    "        if not self.use_distance or not self.normalize_distance and key not in self._distance_probs:\n",
    "            return np.ones((src_length, trg_length)) * 1.0 / src_length\n",
    "        \n",
    "        if not self.normalize_distance:\n",
    "            return self._distance_probs[key]\n",
    "        \n",
    "        result = np.zeros((src_length, trg_length))\n",
    "        for i in range(src_length):\n",
    "            for j in range(trg_length):\n",
    "                src_index = self._clamp(i, src_length)\n",
    "                trg_index = self._clamp(j, trg_length)\n",
    "                result[i][j] = self._distance_probs[src_index][trg_index]\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def get_prior_prob(self, src_index, trg_index, src_length, trg_length):\n",
    "        \"Returns a uniform prior probability.\"\n",
    "        key = (src_length, trg_length)\n",
    "        if not self.use_distance or not self.normalize_distance and key not in self._distance_probs:\n",
    "            return 1.0 / src_length\n",
    "        \n",
    "        if not self.normalize_distance:\n",
    "            return self._distance_probs[key][src_index][trg_index]\n",
    "\n",
    "        return self._distance_probs[self._clamp(src_index, src_length)][self._clamp(trg_index, trg_length)]\n",
    "\n",
    "    def collect_statistics(self, src_length, trg_length, posterior_matrix):\n",
    "        \"Extract the necessary statistics from this matrix if needed.\"\n",
    "        if not self.use_distance:\n",
    "            return\n",
    "        \n",
    "        if not self.normalize_distance:\n",
    "            key = (src_length, trg_length)\n",
    "            if key not in self._distance_counts:\n",
    "                self._distance_counts[key] = np.zeros(key)\n",
    "\n",
    "            self._distance_counts[key] += posterior_matrix.T\n",
    "            return\n",
    "\n",
    "        for i in range(src_length):\n",
    "            for j in range(trg_length):\n",
    "                src_index = self._clamp(i, src_length)\n",
    "                trg_index = self._clamp(j, trg_length)\n",
    "                self._distance_counts[src_index][trg_index] += posterior_matrix[j][i]\n",
    "        \n",
    "    def recompute_parameters(self):\n",
    "        \"Reestimate the parameters and reset counters.\"\n",
    "        if not self.use_distance:\n",
    "            return\n",
    "        \n",
    "        if not self.normalize_distance:\n",
    "            for key in self._distance_counts:\n",
    "                denoms = np.sum(self._distance_counts[key], axis=0, keepdims=True)\n",
    "                self._distance_probs[key] = self._distance_counts[key]/denoms\n",
    "\n",
    "            self._distance_counts = {}\n",
    "            return\n",
    "        \n",
    "        denoms = np.sum(self._distance_counts, axis=0, keepdims=True)\n",
    "        self._distance_probs = self._distance_counts/denoms\n",
    "        self._distance_counts = np.zeros((self.dim, self.dim))\n",
    "\n",
    "class TransitionModel:\n",
    "    \"Models the prior probability of an alignment conditioned on previous alignment.\"\n",
    "\n",
    "    def __init__(self, src_corpus, trg_corpus):\n",
    "        \"Add counters and parameters here for more sophisticated models.\"\n",
    "        pass\n",
    "\n",
    "    def get_parameters_for_sentence_pair(self, src_length):\n",
    "        \"Retrieve the parameters for this sentence pair: A[k, i] = p(a_{j} = i|a_{j-1} = k)\"\n",
    "        pass\n",
    "\n",
    "    def collect_statistics(self, src_length, bigram_posteriors):\n",
    "        \"Extract statistics from the bigram posterior[i][j]: p(a_{t-1} = i, a_{t} = j| e, f)\"\n",
    "        pass\n",
    "        \n",
    "    def recompute_parameters(self):\n",
    "        \"Recompute the transition matrix\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/michmech/lemmatization-lists/raw/master/lemmatization-en.txt\n",
    "!wget https://github.com/michmech/lemmatization-lists/raw/master/lemmatization-cs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_lemmas(file):\n",
    "    lemmas = {}\n",
    "    with open(file, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            lemma, word = line.strip().split('\\t')\n",
    "            lemmas[word] = lemma\n",
    "    return lemmas\n",
    "\n",
    "lemmas_en = load_lemmas('lemmatization-en.txt')\n",
    "lemmas_cs = load_lemmas('lemmatization-cs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the framework for training and evaluating a model using EM.\n",
    "\n",
    "from utils import read_parallel_corpus, extract_test_set_alignments, score_alignments, write_aligned_corpus\n",
    "from itertools import chain\n",
    "\n",
    "def normalize_word(word, lemmas, n_trim):\n",
    "    word = word.lower()\n",
    "    if word not in lemmas:\n",
    "        return word[:n_trim]\n",
    "    return lemmas[word][:n_trim]\n",
    "\n",
    "def normalize_sentence(sentence, lemmas, n_trim):\n",
    "    return list(map(lambda x: normalize_word(x, lemmas, n_trim), sentence))\n",
    "\n",
    "def infer_posteriors(src_tokens, trg_tokens, prior_model, translation_model):\n",
    "    \"Compute the posterior probability p(a_j=i | f, e) for each target token f_j given e and f.\"\n",
    "    # HINT: An HMM will require more complex statistics over the hidden alignments.\n",
    "    P = prior_model.get_parameters_for_sentence_pair(len(src_tokens), len(trg_tokens)) # p[i][j] = P(a_ij)\n",
    "    T = translation_model.get_parameters_for_sentence_pair(src_tokens, trg_tokens) # t[i][j] = P(f_j|e_i)\n",
    "    posteriors = P*T\n",
    "    marginals = np.sum(posteriors, axis=0)\n",
    "    posteriors /= marginals\n",
    "    log_likelihood = np.sum(np.log(marginals))\n",
    "    return posteriors.T, log_likelihood\n",
    "\n",
    "def collect_expected_statistics(src_corpus, trg_corpus, prior_model, translation_model):\n",
    "    \"E-step: infer posterior distribution over each sentence pair and collect statistics.\"\n",
    "    corpus_log_likelihood = 0.0\n",
    "    for src_tokens, trg_tokens in zip(src_corpus, trg_corpus):\n",
    "        # Infer posterior\n",
    "        posteriors, log_likelihood = infer_posteriors(src_tokens, trg_tokens, prior_model, translation_model)\n",
    "        # Collect statistics in each model.\n",
    "        prior_model.collect_statistics(len(src_tokens), len(trg_tokens), posteriors)\n",
    "        translation_model.collect_statistics(src_tokens, trg_tokens, posteriors)\n",
    "        # Update log prob\n",
    "        corpus_log_likelihood += log_likelihood\n",
    "    return corpus_log_likelihood\n",
    "\n",
    "def estimate_models(src_corpus, trg_corpus, prior_model, translation_model, num_iterations):\n",
    "    \"Estimate models iteratively using EM.\"\n",
    "    for iteration in range(num_iterations):\n",
    "        # E-step\n",
    "        corpus_log_likelihood = collect_expected_statistics(src_corpus, trg_corpus, prior_model, translation_model)\n",
    "        # M-step\n",
    "        prior_model.recompute_parameters()\n",
    "        translation_model.recompute_parameters()\n",
    "        if iteration > 0:\n",
    "            print(\"corpus log likelihood: %1.3f\" % corpus_log_likelihood)\n",
    "    return prior_model, translation_model\n",
    "\n",
    "def get_alignments_from_posterior(posteriors):\n",
    "    \"Returns the MAP alignment for each target word given the posteriors.\"\n",
    "    # HINT: If you implement an HMM, you may want to implement a better algorithm here.\n",
    "    alignments = {}\n",
    "    for src_index, trg_index in enumerate(np.argmax(posteriors, 1)):\n",
    "        if src_index not in alignments:\n",
    "            alignments[src_index] = {}\n",
    "        alignments[src_index][trg_index] = '*'\n",
    "    return alignments\n",
    "\n",
    "def align_corpus(src_corpus, trg_corpus, prior_model, translation_model):\n",
    "    \"Align each sentence pair in the corpus in turn.\"\n",
    "    aligned_corpus = []\n",
    "    for src_tokens, trg_tokens in zip(src_corpus, trg_corpus):\n",
    "        posteriors, _ = infer_posteriors(src_tokens, trg_tokens, prior_model, translation_model)\n",
    "        alignments = get_alignments_from_posterior(posteriors)\n",
    "        aligned_corpus.append((src_tokens, trg_tokens, alignments))\n",
    "    return aligned_corpus\n",
    "\n",
    "def initialize_models(src_corpus, trg_corpus):\n",
    "    prior_model = PriorModel(src_corpus, trg_corpus)\n",
    "    translation_model = TranslationModel(src_corpus, trg_corpus)\n",
    "    return prior_model, translation_model\n",
    "\n",
    "def normalize(src_corpus, trg_corpus, norm, n_params, n_trim):\n",
    "    if norm:\n",
    "        normalized_src = list(map(lambda x: normalize_sentence(x, lemmas_en, n_trim), src_corpus))\n",
    "        normalized_trg = list(map(lambda x: normalize_sentence(x, lemmas_cs, n_trim), trg_corpus))\n",
    "\n",
    "        if n_params:\n",
    "            src_unique = set(chain.from_iterable(normalized_src))\n",
    "            trg_unique = set(chain.from_iterable(normalized_trg))\n",
    "            \n",
    "            src_mapping = {word: i%n_params for i, word in enumerate(src_unique)}\n",
    "            trg_mapping = {word: i%n_params for i, word in enumerate(trg_unique)}\n",
    "            \n",
    "            hash_src = lambda src_sentence: list(map(lambda word: src_mapping[word]%n_params, src_sentence))\n",
    "            hash_trg = lambda trg_sentence: list(map(lambda word: trg_mapping[word]%n_params, trg_sentence))\n",
    "            \n",
    "            normalized_src = list(map(hash_src, normalized_src))\n",
    "            normalized_trg = list(map(hash_trg, normalized_trg))\n",
    "    \n",
    "        return normalized_src, normalized_trg\n",
    "    return src_corpus, trg_corpus\n",
    "\n",
    "def train(num_iterations, norm=False, n_params=None, n_trim=None):\n",
    "    src_corpus, trg_corpus, _ = read_parallel_corpus('en-cs.all')\n",
    "    src_corpus, trg_corpus = normalize(src_corpus, trg_corpus, norm, n_params, n_trim)\n",
    "    prior_model, translation_model = initialize_models(src_corpus, trg_corpus)\n",
    "    prior_model, translation_model = estimate_models(src_corpus, trg_corpus, prior_model, translation_model, num_iterations)    \n",
    "    aligned_corpus = align_corpus(src_corpus, trg_corpus, prior_model, translation_model)\n",
    "    return aligned_corpus, extract_test_set_alignments(aligned_corpus)\n",
    "\n",
    "def evaluate(candidate_alignments):\n",
    "    src_dev, trg_dev, wa_dev = read_parallel_corpus('en-cs-wa.dev', has_alignments=True)\n",
    "    src_test, trg_test, wa_test = read_parallel_corpus('en-cs-wa.test', has_alignments=True)\n",
    "    print('recall %1.3f; precision %1.3f; aer %1.3f' % score_alignments(wa_dev, candidate_alignments['dev']))\n",
    "    print('recall %1.3f; precision %1.3f; aer %1.3f' % score_alignments(wa_test, candidate_alignments['test']))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала посмотрим на результат с обычным PriorModel и без нормализации текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1381305.485\n",
      "corpus log likelihood: -1234633.856\n",
      "corpus log likelihood: -1169520.317\n",
      "corpus log likelihood: -1143343.672\n",
      "recall 0.483; precision 0.434; aer 0.544\n",
      "recall 0.480; precision 0.426; aer 0.550\n"
     ]
    }
   ],
   "source": [
    "aligned_corpus, test_alignments = train(5)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нормализуем текст (лемматизация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1356617.212\n",
      "corpus log likelihood: -1184343.907\n",
      "corpus log likelihood: -1112444.266\n",
      "corpus log likelihood: -1087090.923\n",
      "recall 0.567; precision 0.506; aer 0.466\n",
      "recall 0.564; precision 0.498; aer 0.472\n"
     ]
    }
   ],
   "source": [
    "aligned_corpus, test_alignments = train(5, norm=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем IBM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1356617.212\n",
      "corpus log likelihood: -1049028.146\n",
      "corpus log likelihood: -833314.033\n",
      "corpus log likelihood: -724005.932\n",
      "recall 0.592; precision 0.535; aer 0.439\n",
      "recall 0.600; precision 0.536; aer 0.435\n"
     ]
    }
   ],
   "source": [
    "PriorModel.use_distance = True\n",
    "aligned_corpus, test_alignments = train(5, norm=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем IBM Model 2 с нормализацей расстояния, в этой модели мы избавляемся от зависимости вероятностей от длины предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1077662.585\n",
      "corpus log likelihood: -875468.871\n",
      "corpus log likelihood: -744753.776\n",
      "corpus log likelihood: -673244.557\n",
      "recall 0.664; precision 0.606; aer 0.367\n",
      "recall 0.673; precision 0.607; aer 0.363\n"
     ]
    }
   ],
   "source": [
    "PriorModel.use_distance = True\n",
    "PriorModel.normalize_distance = True\n",
    "aligned_corpus, test_alignments = train(5, norm=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого слова при нормализации оставим только $n$ букв. Сначала хотел сделать это только для тех слов, для которых не нашлось нормальной формы в словарике, но оказалось, что качество улучшается если проделать это для всех слов. Далее подберем такое n, при котором получим наименьший AER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1095914.553\n",
      "corpus log likelihood: -897770.427\n",
      "corpus log likelihood: -764167.632\n",
      "corpus log likelihood: -694624.096\n",
      "recall 0.714; precision 0.651; aer 0.320\n",
      "recall 0.710; precision 0.640; aer 0.328\n"
     ]
    }
   ],
   "source": [
    "PriorModel.use_distance = True\n",
    "PriorModel.normalize_distance = True\n",
    "aligned_corpus, test_alignments = train(5, norm=True, n_trim=4)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1086179.937\n",
      "corpus log likelihood: -875310.607\n",
      "corpus log likelihood: -736549.810\n",
      "corpus log likelihood: -666861.825\n",
      "recall 0.710; precision 0.647; aer 0.324\n",
      "recall 0.711; precision 0.641; aer 0.327\n"
     ]
    }
   ],
   "source": [
    "PriorModel.use_distance = True\n",
    "PriorModel.normalize_distance = True\n",
    "aligned_corpus, test_alignments = train(5, norm=True, n_trim=5)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1081635.925\n",
      "corpus log likelihood: -870495.182\n",
      "corpus log likelihood: -732633.979\n",
      "corpus log likelihood: -662343.348\n",
      "recall 0.697; precision 0.635; aer 0.337\n",
      "recall 0.702; precision 0.632; aer 0.336\n"
     ]
    }
   ],
   "source": [
    "PriorModel.use_distance = True\n",
    "PriorModel.normalize_distance = True\n",
    "aligned_corpus, test_alignments = train(5, norm=True, n_trim=6)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем при нормализации еще уменьшить количество параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1138015.153\n",
      "corpus log likelihood: -1014288.493\n",
      "corpus log likelihood: -906801.819\n",
      "corpus log likelihood: -834761.967\n",
      "recall 0.681; precision 0.627; aer 0.349\n",
      "recall 0.683; precision 0.620; aer 0.352\n"
     ]
    }
   ],
   "source": [
    "PriorModel.use_distance = True\n",
    "PriorModel.normalize_distance = True\n",
    "aligned_corpus, test_alignments = train(5, norm=True, n_params=1000, n_trim=5)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1106181.472\n",
      "corpus log likelihood: -903101.507\n",
      "corpus log likelihood: -765358.626\n",
      "corpus log likelihood: -695055.993\n",
      "recall 0.705; precision 0.643; aer 0.329\n",
      "recall 0.708; precision 0.638; aer 0.330\n"
     ]
    }
   ],
   "source": [
    "PriorModel.use_distance = True\n",
    "PriorModel.normalize_distance = True\n",
    "aligned_corpus, test_alignments = train(5, norm=True, n_params=5000, n_trim=5)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1085561.621\n",
      "corpus log likelihood: -875220.415\n",
      "corpus log likelihood: -736552.567\n",
      "corpus log likelihood: -666811.046\n",
      "recall 0.710; precision 0.647; aer 0.324\n",
      "recall 0.711; precision 0.640; aer 0.327\n"
     ]
    }
   ],
   "source": [
    "PriorModel.use_distance = True\n",
    "PriorModel.normalize_distance = True\n",
    "aligned_corpus, test_alignments = train(5, norm=True, n_params=10000, n_trim=5)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение `n_params` не дало прироста качества. Попробуем теперь поменять параметр `dim` в PriorModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1263342.805\n",
      "corpus log likelihood: -1043689.705\n",
      "corpus log likelihood: -896492.498\n",
      "corpus log likelihood: -823352.346\n",
      "recall 0.713; precision 0.651; aer 0.321\n",
      "recall 0.718; precision 0.645; aer 0.322\n"
     ]
    }
   ],
   "source": [
    "PriorModel.use_distance = True\n",
    "PriorModel.normalize_distance = True\n",
    "PriorModel.dim = 20\n",
    "aligned_corpus, test_alignments = train(5, norm=True, n_trim=5)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1321161.002\n",
      "corpus log likelihood: -1099956.638\n",
      "corpus log likelihood: -951305.541\n",
      "corpus log likelihood: -878282.629\n",
      "recall 0.714; precision 0.653; aer 0.319\n",
      "recall 0.722; precision 0.649; aer 0.318\n"
     ]
    }
   ],
   "source": [
    "PriorModel.use_distance = True\n",
    "PriorModel.normalize_distance = True\n",
    "PriorModel.dim = 25\n",
    "aligned_corpus, test_alignments = train(5, norm=True, n_trim=5)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1365392.053\n",
      "corpus log likelihood: -1142171.810\n",
      "corpus log likelihood: -992404.852\n",
      "corpus log likelihood: -920019.406\n",
      "recall 0.713; precision 0.651; aer 0.321\n",
      "recall 0.717; precision 0.646; aer 0.322\n"
     ]
    }
   ],
   "source": [
    "PriorModel.use_distance = True\n",
    "PriorModel.normalize_distance = True\n",
    "PriorModel.dim = 30\n",
    "aligned_corpus, test_alignments = train(5, norm=True, n_trim=5)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге достигли `AER=0.318`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'corpus_reader.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# To visualize aligned corpus:\n",
    "# 1. call write_aligned_corpus(aligned_corpus, 'out')\n",
    "# 2. run python corpus_browser.py en-cs-wa.out (in working directory)\n",
    "\n",
    "write_aligned_corpus(aligned_corpus, 'out')\n",
    "!python corpus_reader.py en-cs-wa.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discrete HMM with scaling. You may want to use this if you decide to implement an HMM.\n",
    "# The parameters for this HMM will still need to be provided by the models above.\n",
    "\n",
    "def forward(pi, A, O):\n",
    "    S, T = O.shape\n",
    "    alpha = np.zeros((S, T))\n",
    "    scaling_factors = np.zeros(T)\n",
    "    \n",
    "    # base case\n",
    "    alpha[:, 0] = pi * O[:, 0]\n",
    "    scaling_factors[0] = np.sum(alpha[:, 0])\n",
    "    alpha[:, 0] /= scaling_factors[0] \n",
    "    \n",
    "    # recursive case\n",
    "    for t in range(1, T):\n",
    "        alpha[:, t] = np.dot(alpha[:, t-1], A[:, :]) * O[:, t]\n",
    "\n",
    "        # Normalize at each step to prevent underflow.\n",
    "        scaling_factors[t] = np.sum(alpha[:, t])\n",
    "        alpha[:, t] /= scaling_factors[t]\n",
    "\n",
    "    return (alpha, scaling_factors)\n",
    "\n",
    "def backward(pi, A, O, forward_scaling_factors):\n",
    "    S, T = O.shape\n",
    "    beta = np.zeros((S, T))\n",
    "\n",
    "    # base case\n",
    "    beta[:, T-1] = 1 / forward_scaling_factors[T-1]\n",
    "    \n",
    "    # recursive case\n",
    "    for t in range(T-2, -1, -1):\n",
    "        beta[:, t] = np.sum(beta[:, t+1] * A[:, :] * O[:, t+1], 1) / forward_scaling_factors[t]\n",
    "\n",
    "    return beta\n",
    "\n",
    "def forward_backward(pi, A, O):\n",
    "    alpha, forward_scaling_factors = forward(pi, A, O)\n",
    "    beta = backward(pi, A, O, forward_scaling_factors)\n",
    "    return alpha, beta, np.sum(np.log(forward_scaling_factors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
